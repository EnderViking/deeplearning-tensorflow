{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Tensorflow 编程基础</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Tensorflow中简单的交互机制 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello world!'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant(\"hello world!\")\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相乘:  12\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(3)\n",
    "b = tf.constant(4)\n",
    "with tf.Session() as sess:\n",
    "    print(\"相乘: \",sess.run(a*b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 注入机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "相加: 7.0\n",
      "相乘: 20.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(dtype=tf.float32)\n",
    "b = tf.placeholder(dtype=tf.float32)\n",
    "add = tf.add(a,b)\n",
    "mul = tf.multiply(a,b)\n",
    "with tf.Session() as sess:\n",
    "    print(\"相加:\",sess.run(add,feed_dict={a:3,b:4}))\n",
    "    print(\"相乘:\",sess.run(mul,feed_dict={a:5,b:4}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 保存和载入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model/2_model_save.cpkt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 要保存的变量不能和前面的重名\n",
    "w1 = tf.placeholder(dtype=tf.float32, name=\"w1\")\n",
    "w2 = tf.placeholder(dtype = tf.float32, name=\"w2\")\n",
    "b1= tf.Variable(2.0,name=\"bias\")\n",
    "\n",
    "# 对于需要模型中需要取出来继续用的变量,需要指定名字\n",
    "w3 = tf.add(w1,w2)\n",
    "w4 = tf.multiply(w3,b1,name=\"op_to_restore\")\n",
    "\n",
    "# 通过最简单的交互式方式保存模型\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#Run the operation by feeding input\n",
    "print(sess.run(w4,feed_dict ={w1:4,w2:8}))\n",
    "\n",
    "saver.save(sess, 'model/2_model_save.cpkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/2_model_save.cpkt\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "sess2 = tf.Session()\n",
    "sess2.run(tf.global_variables_initializer())\n",
    "saver.restore(sess2,'model/2_model_save.cpkt')\n",
    "print(sess2.run(w4,feed_dict={w1:2,w2:4}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/2_model_save.cpkt\n"
     ]
    }
   ],
   "source": [
    "sess3 = tf.Session()\n",
    "sess3.run(tf.global_variables_initializer())\n",
    "ckpt =  tf.train.get_checkpoint_state('model/')\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess3,ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 输出模型的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_name:  bias\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "print_tensors_in_checkpoint_file('model/2_model_save.cpkt',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 TensorBoard 的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x = np.linspace(-1,1,100)\n",
    "data_y = 2*data_x + np.random.randn(100)*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_ = np.linspace(1,2,10)\n",
    "y_ = 2*x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'z:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "w = tf.Variable(tf.random_normal(shape=[1]),dtype=tf.float32)\n",
    "b = tf.Variable(tf.zeros(shape=[1]),dtype=tf.float32)\n",
    "z = tf.multiply(w,x)+b\n",
    "tf.summary.histogram('z',z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(z-y))\n",
    "tf.summary.scalar('loss',cost)\n",
    "learning_rate = 0.01\n",
    "epochs = 20\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 4.779634\n",
      "epoch: 2 loss: 0.22657487\n",
      "epoch: 4 loss: 0.002929587\n",
      "epoch: 6 loss: 0.0038391806\n",
      "epoch: 8 loss: 0.007982647\n",
      "epoch: 10 loss: 0.009315145\n",
      "epoch: 12 loss: 0.009677233\n",
      "epoch: 14 loss: 0.009772141\n",
      "epoch: 16 loss: 0.009796505\n",
      "epoch: 18 loss: 0.0098026935\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # 合并所有的 summary\n",
    "    merge_summary_op = tf.summary.merge_all()\n",
    "    # 创建 summary_writer ,用于写文件\n",
    "    summary_writer = tf.summary.FileWriter('log/2_model_summaries',sess.graph)\n",
    "    for i in range(epochs):\n",
    "        for (batch_x,batch_y) in zip(data_x,data_y):\n",
    "            sess.run(optimizer,feed_dict={x:batch_x,y:batch_y})\n",
    "            # 生成 summary\n",
    "            summary_str = sess.run(merge_summary_op,feed_dict={x:batch_x,y:batch_y})\n",
    "            summary_writer.add_summary(summary_str,i)\n",
    "            \n",
    "        if i % 2 == 0:\n",
    "            loss = sess.run(cost,feed_dict={x:x_,y:y_})\n",
    "            print(\"epoch:\",i,\"loss:\",loss)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 张量及操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以把tensor看为一个n维的数组或列表,每个tensor中包含了类型(type),阶(rank)和形状(shape). \n",
    "+ 在TensorFlow中有几层中括号就是几阶. 例如 C=[[1,2],[3,4]] 表示2阶\n",
    "\n",
    "+ 在TensorFlow中使用shape来表示张量的形状,使用列表或元组来表示.例如上面的可以表示为shape=(2,3)或shape=[2,2]\n",
    "\n",
    "+ <font color='red'>Tensorflow所有函数参数都是有名称的,并且名称位置不同,所以建议都带上参数名</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'1.0', 1.0, 1.0, 1, 1.0]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(\"1.0\",dtype=tf.string)\n",
    "b = tf.string_to_number(a)\n",
    "c = tf.to_float(b)\n",
    "d = tf.to_int32(b)\n",
    "e = tf.cast(d,dtype=tf.float32)\n",
    "sess = tf.Session()\n",
    "print(sess.run([a,b,c,d,e]))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 数值操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [[1 1 1]\n",
      " [1 1 1]]\n",
      "b: [[0 0 0]\n",
      " [0 0 0]]\n",
      "c: [[1 1 1]\n",
      " [1 1 1]]\n",
      "d: [[2 2 2]\n",
      " [2 2 2]]\n",
      "e: [[5 5 5]\n",
      " [5 5 5]]\n",
      "f: [[-0.24933764 -0.552097   -0.75433433]\n",
      " [-1.1415269  -1.6429098  -0.4253484 ]]\n",
      "g: [[-0.46005023 -0.8525768  -0.49750695]\n",
      " [ 0.9802293  -1.2426956   1.0782707 ]]\n",
      "h: [[0.0759635  0.83476555 0.4947499 ]\n",
      " [0.41882873 0.01814735 0.7201973 ]]\n",
      "i: [ 1.    3.25  5.5   7.75 10.  ]\n",
      "j: [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones(shape=[2,3],dtype=tf.int32)\n",
    "b = tf.zeros(shape=[2,3],dtype=tf.int32)\n",
    "c = tf.ones_like(b,dtype=tf.int32)\n",
    "d = tf.fill(dims=[2,3],value=2)\n",
    "e = tf.constant(shape=[2,3],value=5)\n",
    "# 为了保证每次随机数相同,所以可以设置随机数\n",
    "tf.set_random_seed(42)\n",
    "# tf.random_normal 表示正态分布随机数,均值为mean,标准差为stddev\n",
    "f = tf.random_normal(shape=[2,3],dtype=tf.float32)\n",
    "# tf.truncated_normal 表示截断正态分布随机数,保留[mean-2*stddev,mean+2*stddev]\n",
    "g = tf.truncated_normal(shape=[2,3],dtype=tf.float32)\n",
    "# tf.random_uniform 表示均匀分布随机数,范围[minval,maxval]\n",
    "h = tf.random_uniform(shape=[2,3],minval=0.0,maxval=1.0,dtype=tf.float32)\n",
    "# 产生等差数列,start 需要使用浮点数,包括stop\n",
    "i = tf.linspace(start=1.0,stop=10.0,num=5)\n",
    "# 产生等差数列,start 可以用整数,不包括limit\n",
    "j = tf.range(start=1,limit=5,delta=1)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('a:',sess.run(a))\n",
    "print('b:',sess.run(b))\n",
    "print('c:',sess.run(c))\n",
    "print('d:',sess.run(d))\n",
    "print('e:',sess.run(e))\n",
    "print('f:',sess.run(f))\n",
    "print('g:',sess.run(g))\n",
    "print('h:',sess.run(h))\n",
    "print('i:',sess.run(i))\n",
    "print('j:',sess.run(j))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 形状变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "t = np.arange(1,10)\n",
    "print(np.shape(t))\n",
    "t_shape = tf.shape(t)\n",
    "sess = tf.Session()\n",
    "print(sess.run(t_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# tf.size 返回一个张量,表示输入数据元素的个数\n",
    "t_size = tf.size(t)\n",
    "print(sess.run(t_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "t_rank = tf.rank(t)\n",
    "print(sess.run(t_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "t_reshape = tf.reshape(t,shape=[3,3])\n",
    "print(sess.run(t_reshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-5b3dd965d687>:5: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "(2, 3)\n",
      "(1, 2, 3)\n",
      "(2, 1, 3)\n",
      "(2, 3, 1)\n",
      "(2, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "t = [[2,3,3],[1,5,5]]\n",
    "# 扩展维度就是在指定dim位置增加一个维度\n",
    "# 通常而言,axis和dim的作用相同,在新版本中大多使用axis\n",
    "t0 = tf.expand_dims(input=t,axis=0)\n",
    "t1 = tf.expand_dims(input=t,dim=1)\n",
    "t2 = tf.expand_dims(input=t,dim=2)\n",
    "# 扩展最后一个维度\n",
    "t2_ = tf.expand_dims(input=t,dim=-1)\n",
    "print(np.shape(t))\n",
    "print(np.shape(t0))\n",
    "print(np.shape(t1))\n",
    "print(np.shape(t2))\n",
    "print(np.shape(t2_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2, 1)\n",
      "(1, 2, 1)\n",
      "(1, 2, 1)\n",
      "(1, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# tf.squeeze 将dim指定的维度去掉,并且dim指定的维度必须为1\n",
    "# 通常而言,axis和dim的作用相同,在新版本中大多使用axis\n",
    "t = [[[[2],[1]]]]\n",
    "print(np.shape(t))\n",
    "t0 = tf.squeeze(input=t,axis=0)\n",
    "t1 = tf.squeeze(input=t,axis=1)\n",
    "t3 = tf.squeeze(input=t,axis=3)\n",
    "# 去掉第二维度会报错\n",
    "print(np.shape(t0))\n",
    "print(np.shape(t1))\n",
    "print(np.shape(t3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 数据操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n",
      "[[[3 3 3]]]\n"
     ]
    }
   ],
   "source": [
    "t = [[[1,1,1],[2,2,2]],[[3,3,3],[4,4,4]],[[5,5,5],[6,6,6]]]\n",
    "print(np.shape(t))\n",
    "# begin 表示从哪里开始切,size 表示切多大\n",
    "slice_t1 = tf.slice(input_=t,begin=[1,0,0],size=[1,1,3])\n",
    "print(sess.run(slice_t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 1]\n",
      "[3 2 2]\n"
     ]
    }
   ],
   "source": [
    "# tf.split 表示将tensor沿着某一个轴切分成几个tensor\n",
    "split_0,split_1 = tf.split(value=t,num_or_size_splits=[1,2],axis=2)\n",
    "print(sess.run(tf.shape(split_0)))\n",
    "print(sess.run(tf.shape(split_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "[[ 1  2  3  7  8  9]\n",
      " [ 4  5  6 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "# tf.concat 表示将两个tensor沿着某一个维度拼接,其实这里的 axis 和前面的 dim 是一样的 \n",
    "t1 = [[1,2,3],[4,5,6]]\n",
    "t2 = [[7,8,9],[10,11,12]]\n",
    "concat_0 = tf.concat(values=[t1,t2],axis=0)\n",
    "concat_1 = tf.concat(values=[t1,t2],axis=1)\n",
    "print(sess.run(concat_0))\n",
    "print(sess.run(concat_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]]\n",
      "[[[ 1  2  3]\n",
      "  [ 7  8  9]]\n",
      "\n",
      " [[ 4  5  6]\n",
      "  [10 11 12]]]\n",
      "[[[ 1  7]\n",
      "  [ 2  8]\n",
      "  [ 3  9]]\n",
      "\n",
      " [[ 4 10]\n",
      "  [ 5 11]\n",
      "  [ 6 12]]]\n"
     ]
    }
   ],
   "source": [
    "# tf.stack 将两个N维张量列表,沿着axis轴,组合成一个沿着axis轴的N+1维的张量\n",
    "# 其功能和 tf.expand_dims 非常相似,都是在指定 axis 增加一个维度,并且将两个tensor按照这个维度拼接\n",
    "# 总结,先按照 axis 增加维度,再按照该维度拼接\n",
    "stack_0 = tf.stack(values=[t1,t2],axis=0)\n",
    "stack_1 = tf.stack(values=[t1,t2],axis=1)\n",
    "stack_2 = tf.stack(values=[t1,t2],axis=2)\n",
    "print(sess.run(stack_0))\n",
    "print(sess.run(stack_1))\n",
    "print(sess.run(stack_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[array([1, 2, 3], dtype=int32), array([4, 5, 6], dtype=int32)]\n",
      "[array([1, 4], dtype=int32), array([2, 5], dtype=int32), array([3, 6], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "# tf.unstack 表示将输入tensor按照指定的axis拆分\n",
    "# 和 tf.split 非常相似, tf.split 可以拆分成大小不同的tensor\n",
    "t = [[1,2,3],[4,5,6]]\n",
    "print(np.shape(t))\n",
    "unstack_0 = tf.unstack(value=t,axis=0)\n",
    "unstack_1 = tf.unstack(value=t,axis=1)\n",
    "print(sess.run(unstack_0))\n",
    "print(sess.run(unstack_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2]\n"
     ]
    }
   ],
   "source": [
    "# tf.gather获取tensor中指定索引的的元素\n",
    "x = tf.constant(value=[1,2,3,4],dtype=tf.int32)\n",
    "y = tf.gather(params=x,indices=[3,1])\n",
    "print(sess.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# tf.one_hot 用于生成one_hot编码,在tensorflow实现word2vec中非常实用\n",
    "x = [1,2,3,4]\n",
    "depth = 4\n",
    "one_hot = tf.one_hot(indices=x,depth=depth,on_value=1,off_value=0,axis=-1,dtype=tf.int32)\n",
    "print(sess.run(one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# tf.count_nonzero 用于统计非0个数\n",
    "non_zero = tf.count_nonzero(input_tensor=one_hot)\n",
    "print(sess.run(non_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) 算术运算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# tf.assign 赋值函数,必须要运行之后才能使用\n",
    "x = tf.Variable(initial_value=tf.constant(10),dtype=tf.int32)\n",
    "y = tf.constant(value=1,dtype=tf.int32)\n",
    "# 关键是需要运行tf.assign(x,y)\n",
    "sess.run(tf.assign(x,y))\n",
    "print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "-1\n",
      "2\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# 基本的加减乘除运算\n",
    "x = tf.constant(value=1,dtype=tf.int32)\n",
    "y = tf.constant(value=2,dtype=tf.int32)\n",
    "a = tf.add(x,y)\n",
    "b = tf.subtract(x,y)\n",
    "c = tf.multiply(x,y)\n",
    "# tensorflow 中的除法是真除法,会产生小数\n",
    "d = tf.divide(x,y)\n",
    "print(sess.run(a))\n",
    "print(sess.run(b))\n",
    "print(sess.run(c))\n",
    "print(sess.run(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "-2\n",
      "1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(value=2,dtype=tf.int32)\n",
    "y = tf.constant(value=3,dtype=tf.int32)\n",
    "a = tf.mod(x,y)\n",
    "b = tf.abs(x)\n",
    "c = tf.negative(x)\n",
    "d = tf.sign(x)\n",
    "e = tf.square(x)\n",
    "print(sess.run(a))\n",
    "print(sess.run(b))\n",
    "print(sess.run(c))\n",
    "print(sess.run(d))\n",
    "print(sess.run(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  2.  2. -4.]\n",
      "[0.94868326 1.5811388  1.5165751  1.2247449         nan]\n",
      "[2.4596031e+00 1.2182494e+01 9.9741821e+00 4.4816890e+00 1.1108996e-02]\n",
      "[-0.10536055  0.91629076  0.8329091   0.4054651          nan]\n",
      "[1.  2.5 2.3 1.5 1. ]\n"
     ]
    }
   ],
   "source": [
    "x = [0.9,2.5,2.3,1.5,-4.5]\n",
    "# tf.round 表示舍入到最近的整数\n",
    "a = tf.round(x)\n",
    "b = tf.sqrt(x)\n",
    "# tf.exp计算e的x次方\n",
    "c = tf.exp(x)\n",
    "# tf.log 默认计算e的对数,第二个参数指定底数\n",
    "d = tf.log(x)\n",
    "# tf.maximum 返回最大值,并且需要设置一个默认值\n",
    "e = tf.maximum(x,1.0)\n",
    "print(sess.run(a))\n",
    "print(sess.run(b))\n",
    "print(sess.run(c))\n",
    "print(sess.run(d))\n",
    "print(sess.run(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# tf.cond 是tensorflow中非常有特色的计算\n",
    "x = tf.constant(value=2,dtype=tf.int32)\n",
    "y = tf.constant(value=3,dtype=tf.int32)\n",
    "def f1():\n",
    "    return tf.multiply(x,2)\n",
    "def f2():\n",
    "    return tf.multiply(y,3)\n",
    "rst = tf.cond(tf.less(x,y),true_fn=f1,false_fn=f2)\n",
    "print(sess.run(rst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) 矩阵相关的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 2 0]\n",
      " [0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "# tf.diag 给定对角值,返回对角矩阵\n",
    "diag = [1,2,3]\n",
    "a = tf.diag(diagonal=diag)\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# 给定对角矩阵,返回对角值\n",
    "b = tf.diag_part(input=a)\n",
    "print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "# tf.transpose 用于矩阵转置,可以指定转置维度,默认全转置\n",
    "t = [[1,2,3],[4,5,6]]\n",
    "print(np.shape(t))\n",
    "a = tf.transpose(t)\n",
    "print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30.  36.  42.]\n",
      " [ 66.  81.  96.]\n",
      " [102. 126. 150.]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1,10,dtype=np.float32)\n",
    "a = np.reshape(a,newshape=(3,3))\n",
    "b = a\n",
    "c = tf.matmul(a,b)\n",
    "d = tf.matrix_determinant(input=a)\n",
    "print(sess.run(c))\n",
    "print(sess.run(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) 规约计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规约计算操作,都有降维的功能,在所有reduce_xxx 系列操作函数中,都是以xxx手段降维,每个函数都有 axis 这个参数,也就是沿着某个方向,使用 xxx 方法对输入的Tensor 进行降维. axis 默认是 None,也就是把 input_tensor 降到 0 维,即一个数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>tensorflow 中 axis 和 numpy 中 axis 概念是一样的,axis=0 表示跨行计算,axis=1 表示跨列计算</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "[5 7 9]\n",
      "[ 6 15]\n"
     ]
    }
   ],
   "source": [
    "# 计算输入tensor元素的和,或者按照axis指定的轴进行求和\n",
    "x = [[1,2,3],[4,5,6]]\n",
    "a = tf.reduce_sum(input_tensor=x)\n",
    "b = tf.reduce_sum(input_tensor=x,axis=0)\n",
    "c = tf.reduce_sum(input_tensor=x,axis=1)\n",
    "print(sess.run(a))\n",
    "print(sess.run(b))\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n",
      "[ 4 10 18]\n",
      "[  6 120]\n"
     ]
    }
   ],
   "source": [
    "# 计算输入tensor元素的积,或者按照axis指定的轴进行求积\n",
    "x = [[1,2,3],[4,5,6]]\n",
    "a = tf.reduce_prod(input_tensor=x)\n",
    "b = tf.reduce_prod(input_tensor=x,axis=0)\n",
    "c = tf.reduce_prod(input_tensor=x,axis=1)\n",
    "print(sess.run(a))\n",
    "print(sess.run(b))\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似的还有:\n",
    "```python\n",
    "tf.reduce_max\n",
    "tf.reduce_min\n",
    "tf.reduce_mean\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# tf.reduce_all 表示对各个元素求 \"与\"\n",
    "# tf.reduce_any 表示对各个元素求 \"或\"\n",
    "# 输入只能是一些bool值\n",
    "x = [True,False]\n",
    "a = tf.reduce_all(x)\n",
    "b = tf.reduce_any(x)\n",
    "print(sess.run(a))\n",
    "print(sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (8) 序列比较与索引提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "ListDiff(out=array([5], dtype=int32), idx=array([3], dtype=int32))\n",
      "[1 2 3 5]\n",
      "[0 1 2 3]\n",
      "[1 5 3 2]\n"
     ]
    }
   ],
   "source": [
    "x = [1,2,3,5]\n",
    "y = [1,3,4,2]\n",
    "a = tf.argmax(input=x)\n",
    "b = tf.argmin(input=y)\n",
    "# 返回x,y中第一个不同值的索引\n",
    "c = tf.setdiff1d(x,y)\n",
    "# 获取去重后的内容\n",
    "d,inx = tf.unique(x)\n",
    "# tf.random_shuffle 沿着input的第一维进行随机重新排列\n",
    "e = tf.random_shuffle(value=x)\n",
    "print(sess.run(a))\n",
    "print(sess.run(b))\n",
    "print(sess.run(c))\n",
    "print(sess.run(d))\n",
    "print(sess.run(inx))\n",
    "print(sess.run(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 共享变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) tf.Variable() 和 tf.get_variable() 的区别\n",
    "```python\n",
    "tf.Variable() 用于生成一个初始值为 initial-value 的变量,必须指定初始化值\n",
    "W = tf.Variable(<initial-value>,dtype=<optional>,name=<optional-name>)\n",
    "\n",
    "tf.get_variable() 获取已存在的变量(要求不仅名字,而且初始化方法等各个参数都一样),如果不存在,就新建一个. 可以使用各种初始化方法,不需要指定值.\n",
    "W = tf.get_variable(\"W\", shape=[784, 256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "```\n",
    "推荐使用 tf.get_variable(),因为它会检测当前命名空间是否存在同名的变量,可以方便共享变量. 而tf.Variable 每次都会新建一个变量. <font color='red'>使用 get_variable 生成的变量是以指定的name 属性作为唯一标识,并不是定义的变量名称.</font>使用时一般通过 name 属性定位到具体变量,并将其共享到其他模型中\n",
    "\n",
    "#### 需要注意的是,tf.get_variable() 需要配合 reuse 和 tf.variable_scope() 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one/v:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"one\"):\n",
    "    a = tf.get_variable(name=\"v\",shape=[1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    print(a.name)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one/v:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"one\",reuse=tf.AUTO_REUSE):\n",
    "    b = tf.get_variable(name=\"v\",shape=[1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    print(b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从中可以看出,变量a和变量b 是同一个变量,可以将他们放到两个不同的网络中训练,最后的结果都会作用于同一个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two/v:0\n",
      "two/v_1:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"two\"):\n",
    "    d = tf.get_variable(name=\"v\",shape=[1])\n",
    "    print(d.name)\n",
    "    e = tf.Variable(1.0,name='v',expected_shape=[1])\n",
    "    print(e.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_var:0\n",
      "first_var_1:0\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0,name='first_var')\n",
    "b = tf.Variable(2.0,name='first_var')\n",
    "# 不会报错,因为 tf.Variable 每次都会生成一个新变量\n",
    "print(a.name)\n",
    "print(b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable second_var already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-48-0d4206143a5a>\", line 3, in <module>\n    a = tf.get_variable(name='second_var',shape = [1],initializer=tf.constant_initializer(0.3))\n  File \"/home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0d4206143a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# tf.get_variable 不存在时,产生新变量. 新定义的变量,必须指定shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'second_var'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'second_var'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# 会报错,在同一个tf.variable_scope 中,tf.get_variable 同一个变量只能定义一次,可以通过 reuse 来共享\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1328\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1329\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1087\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    741\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 743\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    744\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable second_var already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-48-0d4206143a5a>\", line 3, in <module>\n    a = tf.get_variable(name='second_var',shape = [1],initializer=tf.constant_initializer(0.3))\n  File \"/home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# tf.get_variable 不存在时,产生新变量. 新定义的变量,必须指定shape\n",
    "a = tf.get_variable(name='second_var',shape = [1],initializer=tf.constant_initializer(0.3))\n",
    "b = tf.get_variable(name='second_var',shape = [1],initializer=tf.constant_initializer(0.4))\n",
    "# 会报错,在同一个tf.variable_scope 中,tf.get_variable 同一个变量只能定义一次,可以通过 reuse 来共享\n",
    "print(a.name)\n",
    "print(b.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 作用域与操作符的受限范围"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variable_scope 还可以使用 with variable_scope(\"name\") as xxxscope 的方式定义作用域,当使用这种方式定义作用域,<font color='red'>所定义的作用域 xxxscope 将不在受到外围的 scope 限制</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope1/v:0\n",
      "scope2/v:0\n",
      "scope1/v3:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"scope1\") as sp:\n",
    "    a = tf.get_variable(name=\"v\",shape=[1],dtype=tf.float32)\n",
    "    print(a.name)\n",
    "with tf.variable_scope(\"scope2\"):\n",
    "    b = tf.get_variable(name=\"v\",shape=[1],dtype=tf.float32)\n",
    "    print(b.name)\n",
    "    # 必须使用已经存在的scope才能达到效果\n",
    "    # with tf.variable_scope(\"scope1\") as sp3:\n",
    "    with tf.variable_scope(sp) as sp3:\n",
    "        c = tf.get_variable(name=\"v3\",shape=[1],dtype=tf.float32)\n",
    "        print(c.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 操作符的作用域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 操作op, 受到 tf.name_scope 和 tf.variable_scope 两个作用域限制\n",
    "+ 变量v , 只受到 tf.variable_scope 一个作用域限制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope/v:0\n",
      "scope/bar/add\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"scope\"):\n",
    "    with tf.name_scope(\"bar\"):\n",
    "        v = tf.get_variable(name=\"v\",shape=[1])\n",
    "        x = 1.0 + v\n",
    "# 可以直接在 with 外面访问 变量\n",
    "print(v.name)\n",
    "print(x.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 图的基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 建立图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7f4a52720c50>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f4a52720c50>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f4a52720630>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f4a52720630>\n",
      "<tensorflow.python.framework.ops.Graph object at 0x7f4a52720c88>\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant(value=0.0)\n",
    "# 第一种建图方法\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    c1 = tf.constant(value=0.0)\n",
    "    # 建立在tf.Graph()的基础上\n",
    "    print(c1.graph)\n",
    "    print(g)\n",
    "    # 建立在默认图中\n",
    "    print(c.graph)\n",
    "# 获取默认图\n",
    "g2 = tf.get_default_graph()\n",
    "print(g2)\n",
    "\n",
    "# 重置了默认图,必须保证当前图的资源全部释放,否则会报错.\n",
    "tf.reset_default_graph()\n",
    "g3 = tf.get_default_graph()\n",
    "print(g3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

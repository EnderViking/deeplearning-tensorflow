{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>利用 RNN 训练语言模型</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "gpu_no = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_no\n",
    "# 定义TensorFlow配置\n",
    "config = tf.ConfigProto()\n",
    "# 配置GPU内存分配方式，按需增长，很关键\n",
    "config.gpu_options.allow_growth = True\n",
    "# 配置可使用的显存比例\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "# 在创建session的时候把config作为参数传进去\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 定义基本工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "start_time = time.time()\n",
    "\n",
    "def elapsed(sec):\n",
    "    if sec < 60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec < 60*60:\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \"h\"\n",
    "tf.reset_default_graph()\n",
    "training_file = './data/text/word.txt'\n",
    "\n",
    "# 处理多个中文文件\n",
    "def read_all_txt(txt_files):\n",
    "    words = []\n",
    "    for txt_file in txt_files:\n",
    "        file_words = get_ch_words(txt_file)\n",
    "        words.append(file_words)\n",
    "    return words\n",
    "# 处理汉字\n",
    "def get_ch_words(txt_file):\n",
    "    words = \"\"\n",
    "    with open(txt_file,'rb') as f:\n",
    "        for word in f:\n",
    "            words = words + word.decode('utf-8')\n",
    "    return words\n",
    "\n",
    "# 将文件里面的字符转换成向量\n",
    "def get_ch_words_v(txt_file,word_num_map,txt_words=None):\n",
    "    words_size = len(word_num_map)\n",
    "    to_num = lambda word:word_num_map.get(word,words_size)\n",
    "    if txt_file != None:\n",
    "        txt_words = get_ch_words(txt_file)\n",
    "    txt_words_vector = list(map(to_num,txt_words))\n",
    "    return txt_words_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 样本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这种感觉，愈发地受到强烈震撼，简直超乎了所有想象，就是要将人生之花开出更加丰硕、更加红艳花朵，在三生三世，遍溢余香，掌声雷动，经久不息。浅黄与明黄扉页之上，红红枫叶蹁跹舞蹈，烘焙“枫叶正红”四字，别开生面地飘逸曹树清老先生书集封面，仿佛将曹老神韵，清瘦硬朗，适中身材，熠熠有神眼眸，荡漾精气神十足，仙风道骨，鹤然峭立，把枯藤老树，融化殆尽。使我在拜读他之书作，更是与他心灵相通，灵魂嫁接，老树新花，怒放璀璨绚丽。\n"
     ]
    }
   ],
   "source": [
    "training_data = get_ch_words(training_file)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_num_map:\n",
      "{'仙': 19, '绚': 106, '飘': 137, '熠': 88, '直': 93, '身': 123, '香': 138, '璀': 89, '放': 63, '峭': 46, '有': 69, '四': 36, '书': 15, '殆': 77, '掌': 60, '受': 34, '中': 9, '尽': 45, '息': 51, '红': 104, '灵': 83, '藤': 112, '超': 118, '封': 42, '乎': 14, '佛': 22, '所': 56, '了': 16, '生': 91, '人': 17, '将': 43, '动': 30, '跹': 120, '余': 21, '这': 124, '璨': 90, '感': 54, '我': 55, '鹤': 141, '漾': 82, '然': 87, '荡': 111, '舞': 108, '怒': 50, '丽': 11, '遍': 128, '页': 135, '风': 136, '立': 101, '心': 49, '蹈': 122, '要': 114, '道': 129, '接': 61, '加': 29, '适': 125, '正': 76, '别': 27, '拜': 59, '朗': 70, '融': 113, '”': 1, '三': 4, '就': 44, '愈': 53, '、': 2, '材': 72, '强': 48, '嫁': 40, '字': 41, '浅': 79, '上': 5, '十': 32, '枯': 74, '仿': 20, '与': 7, '面': 133, '声': 39, '韵': 134, '地': 38, '气': 78, '种': 100, '丰': 10, '简': 102, '树': 75, '硬': 98, '枫': 73, '蹁': 121, '集': 130, '通': 126, '雷': 131, '想': 52, '艳': 109, '神': 99, '发': 33, '觉': 115, '曹': 68, '化': 31, '骨': 139, '明': 65, '震': 132, '逸': 127, '眸': 95, '作': 23, '眼': 96, '经': 105, '把': 58, '叶': 35, '新': 64, '不': 6, '读': 116, '烘': 85, '“': 0, '更': 67, '出': 26, '焙': 86, '朵': 71, '开': 47, '烈': 84, '精': 103, '到': 28, '硕': 97, '黄': 142, '。': 3, '他': 18, '使': 24, '扉': 57, '撼': 62, '溢': 81, '魂': 140, '清': 80, '世': 8, '之': 13, '，': 143, '久': 12, '老': 107, '相': 94, '足': 119, '是': 66, '象': 117, '在': 37, '瘦': 92, '花': 110, '先': 25}\n",
      "word list size:\n",
      "144\n",
      "[124, 100, 54, 115, 143, 53, 33, 38, 34, 28, 48, 84, 132, 62, 143, 102, 93, 118, 14, 16, 56, 69, 52, 117, 143, 44, 66, 114, 43, 17, 91, 13, 110, 47, 26, 67, 29, 10, 97, 2, 67, 29, 104, 109, 110, 71, 143, 37, 4, 91, 4, 8, 143, 128, 81, 21, 138, 143, 60, 39, 131, 30, 143, 105, 12, 6, 51, 3, 79, 142, 7, 65, 142, 57, 135, 13, 5, 143, 104, 104, 73, 35, 121, 120, 108, 122, 143, 85, 86, 0, 73, 35, 76, 104, 1, 36, 41, 143, 27, 47, 91, 133, 38, 137, 127, 68, 75, 80, 107, 25, 91, 15, 130, 42, 133, 143, 20, 22, 43, 68, 107, 99, 134, 143, 80, 92, 98, 70, 143, 125, 9, 123, 72, 143, 88, 88, 69, 99, 96, 95, 143, 111, 82, 103, 78, 99, 32, 119, 143, 19, 136, 129, 139, 143, 141, 87, 46, 101, 143, 58, 74, 112, 107, 75, 143, 113, 31, 77, 45, 3, 24, 55, 37, 59, 116, 18, 13, 15, 23, 143, 67, 66, 7, 18, 49, 83, 94, 126, 143, 83, 140, 40, 61, 143, 107, 75, 64, 110, 143, 50, 63, 89, 90, 106, 11, 3]\n"
     ]
    }
   ],
   "source": [
    "couter = Counter(training_data)\n",
    "words = sorted(couter)\n",
    "words_size = len(words)\n",
    "word_num_map = dict(zip(words,range(words_size)))\n",
    "print(\"word_num_map:\")\n",
    "print(word_num_map)\n",
    "print(\"word list size:\")\n",
    "print(words_size)\n",
    "word_v = get_ch_words_v(training_file,word_num_map)\n",
    "print(word_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本例中,使用多层 RNN 模型,后面接一个 softmax 分类, 对下一个字属于哪个向量进行分类,这里认为一个字就是一类."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 设置参数定义占位符\n",
    "学习率设置为 0.001, 迭代次数为 10000 次, 每 1000 次输出一次中间状态. 每次输入 4 个字,来预测第 5 个字. 网络模型使用了 3 层的 LSTM RNN, 第一层为 256 个 cell, 第二层和第三层都是 512 个 cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "learning_rate = 0.001\n",
    "training_iters = 10000\n",
    "display_step = 1000\n",
    "n_input = 4\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 512\n",
    "n_hidden_3 = 512\n",
    "# 定义占位符,其中,x 代表输入的 4 个连续文字,y 代表一个字,由于使用字索引向量的 one_hot 编码,\n",
    "# 所以其大小为 word_size, 代表总共的字数,输入为每个字的 id, 对应的标签是 one_hot 编码.\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,n_input,1])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,words_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 定义网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 x 形状变换并按照时间序列拆分,然后放入 3 层 LSTM 网络,最终通过一个全连接生成 words_size 个节点,为后面的 softmax 做准备."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入数据变成 [batch_size,timesteps]\n",
    "x_1 = tf.reshape(x,[-1,n_input])\n",
    "# 将输入数据变成 [timesteps,batch_size] \n",
    "x_2 = tf.split(x_1,n_input,axis=1)\n",
    "# 2-layer LSTM, 每层有 n_hidden 个 units\n",
    "cell_1 = tf.nn.rnn_cell.LSTMCell(num_units=n_hidden_1)\n",
    "cell_2 = tf.nn.rnn_cell.LSTMCell(num_units=n_hidden_2)\n",
    "cell_3 = tf.nn.rnn_cell.LSTMCell(num_units=n_hidden_3)\n",
    "# 构建多层 RNN 网络\n",
    "rnn_cell = tf.nn.rnn_cell.MultiRNNCell([cell_1,cell_2,cell_3])\n",
    "# 在 static_rnn 网络输入数据的格式必须是 timesteps 优先\n",
    "outputs,states = tf.nn.static_rnn(rnn_cell,x_2,dtype=tf.float32)\n",
    "# 预测的时候,使用全连接层,这一层的神经单元数量与 one_hot 位数相同\n",
    "pred = tf.contrib.layers.fully_connected(outputs[-1],words_size,activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 定义优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 AdamOptimizer 优化器, loss 使用的是 softmax 的交叉熵,正确率是统计 one_hot 中索引对应的位置相同的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义 loss 和 优化器\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# 模型评估\n",
    "correct_pred = tf.equal(tf.argmax(pred,axis=1),tf.argmax(y,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练过程中添加检查点功能,在 session 中每次随机取一个偏移量,然后取后面的4个文字向量当做输入,第五个文字向量当做标签用来计算 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from log/11_rnn_model/rnn_word.cpkt-10000\n",
      "10000\n",
      "Finished!\n",
      "elapsed time:  2.482757568359375 sec\n"
     ]
    }
   ],
   "source": [
    "savedir = \"log/11_rnn_model/\"\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0.0\n",
    "    loss_total = 0.0\n",
    "    kpt = tf.train.latest_checkpoint(savedir)\n",
    "    start_step = 0\n",
    "    if kpt!=None:\n",
    "        saver.restore(sess,kpt)\n",
    "        index = kpt.find(\"-\")\n",
    "        start_step = int(kpt[index+1:])\n",
    "        print(start_step)\n",
    "        step = start_step\n",
    "    while step < training_iters:\n",
    "        # 随机取一个偏移位置\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0,n_input+1)\n",
    "        input_words = [[word_label[i]] for i in range(offset,offset+n_input)]\n",
    "        input_words = np.reshape(np.array(input_words),[-1,n_input,1])\n",
    "        out_onehot = np.zeros([words_size],dtype=np.float32)\n",
    "        out_onehot[word_label[offset+n_input]] = 1.0\n",
    "        out_onehot = np.reshape(out_onehot,[1,-1])\n",
    "        _,acc,lossval,onehot_pred = sess.run([optimizer,accuracy,loss,pred],feed_dict={x:input_words,y:out_onehot})\n",
    "        loss_total += lossval\n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"iter= \" + str(step+1) + \", average loss = \"+ str(loss_total/display_step)+ \", average acc = \" + str(100*acc))\n",
    "            acc_total = 0.0\n",
    "            loss_total = 0.0\n",
    "            input_words_2 = [[word_label[i]] for i in range(offset,offset+n_input)]\n",
    "            out_2 = words[word_label[word_label[offset + n_input]]]\n",
    "            out_pred = words[int(tf.argmax(onehot_pred,axis=1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (input_words_2,out_2,out_pred))\n",
    "            saver.save(sess,savedir + \"rnn_word.cpkt\",global_step = step)\n",
    "        step += 1\n",
    "        offset += n_input + 1\n",
    "    print(\"Finished!\")\n",
    "    saver.save(sess,savedir + \"rnn_word.cpkt\",global_step = step)\n",
    "    print(\"elapsed time: \",elapsed(time.time()-start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

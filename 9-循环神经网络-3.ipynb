{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Tensorflow 实战 RNN</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 单步RNN: RNNCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNCell 它是 Tensorflow 中实现 RNN 的基本单元,每个 RNNCell 都有一个call 方法,使用方式:\n",
    "```python\n",
    "(output,next_state) = call(input,state)\n",
    "```\n",
    "借助图片来说可能更容易理解。假设我们有一个初始状态 $h_0$，还有输入 $x_1$，调用 call($x_1$, $h_0$)后就可以得到($output_1$, $h_1$)：\n",
    "<br/><br/>\n",
    "<center><img src=\"./img/9/1.jpg\" width=\"600\"/></center>\n",
    "<br/><br/>\n",
    "再调用一次 call($x_1$,$h_1$) 就可以得到 ($output_2,h_2$)\n",
    "<br/><br/>\n",
    "<center><img src=\"./img/9/2.jpg\" width=\"600\"/></center>\n",
    "\n",
    "也就是说，每调用一次 RNNCell 的 call 方法，就相当于在时间上“推进了一步”，这就是 RNNCell 的基本功能。除了call方法外，对于RNNCell，还有两个类属性比较重要：\n",
    "+ state_size\n",
    "+ output_size\n",
    "\n",
    "前者是隐层的大小，后者是输出的大小。比如我们通常是将一个 batch 送入模型计算，设输入数据的形状为(batch_size, input_size)，那么计算时得到的隐层状态就是(batch_size, state_size)，输出就是(batch_size, output_size)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "gpu_no = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_no\n",
    "# 定义TensorFlow配置\n",
    "config = tf.ConfigProto()\n",
    "# 配置GPU内存分配方式，按需增长，很关键\n",
    "config.gpu_options.allow_growth = True\n",
    "# 配置可使用的显存比例\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "# 在创建session的时候把config作为参数传进去\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "(32, 128)\n"
     ]
    }
   ],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(num_units=128) # state_size = 128\n",
    "print(cell.state_size) # 128\n",
    "\n",
    "inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\n",
    "h0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态，形状为(batch_size, state_size)\n",
    "output1, h1 = cell.__call__(inputs, h0) #调用call函数\n",
    "\n",
    "print(h1.shape) # (32, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于BasicLSTMCell，情况有些许不同，<font color='red'>因为LSTM可以看做有两个隐状态h和c</font>，对应的隐层就是一个Tuple，每个都是(batch_size, state_size)的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"basic_lstm_cell/Mul_2:0\", shape=(32, 128), dtype=float32)\n",
      "Tensor(\"basic_lstm_cell/Add_1:0\", shape=(32, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)\n",
    "inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\n",
    "h0 = lstm_cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态\n",
    "output, h1 = lstm_cell.__call__(inputs, h0)\n",
    "\n",
    "print(h1.h)  # shape=(32, 128)\n",
    "print(h1.c)  # shape=(32, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 多步RNN: tf.nn.dynamic_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础的 RNNCell 有一个很明显的问题：对于单个的 RNNCell，我们使用它的 call 函数进行运算时，只是在序列时间上前进了一步。比如使用 $x_1$、$h_0$ 得到 $h_1$，通过 $x_2$、$h_1$ 得到 $h_2$ 等。如果我们的序列长度为 10，就要调用 10 次 call 函数，比较麻烦。对此，TensorFlow 提供了一个 $tf.nn.dynamic\\_rnn$ 函数，使用该函数就相当于调用了 $n$ 次 call 函数。即通过 {$h_0,x_1, x_2, …., x_n$} 直接得 {$h_1,h_2…,h_n$}。\n",
    "\n",
    "具体来说，设我们输入数据的格式为 (batch_size, time_steps, input_size) ，其中 time_steps 表示序列本身的长度，如在 Char RNN 中，长度为 10 的句子对应的 time_steps 就等于10。最后的 input_size 就表示输入数据单个序列单个时间维度上固有的长度。另外我们已经定义好了一个 RNNCell，调用该 RNNCell 的 call 函数 time_steps 次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 10)\n",
      "[[-0.84242034  0.14048211  0.940332    0.78976446 -0.03859949  0.72312653\n",
      "   0.17395307  0.3693972  -0.55532706 -0.55590576]\n",
      " [ 0.6636888  -0.9847835  -0.85880333 -0.0067372  -0.46843302 -0.5927951\n",
      "   0.07098266 -0.7169884   0.36924684  0.87033564]\n",
      " [ 0.11519817  0.71050143 -0.47727397  0.76597834 -0.26745605 -0.19413835\n",
      "  -0.3841084  -0.36119905 -0.47373918 -0.98366785]\n",
      " [-0.90471053 -0.49273717  0.5766394  -0.89226353 -0.40110266  0.19063497\n",
      "   0.2493871   0.02936358  0.86817366  0.6109415 ]]\n",
      "(4, 10)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 4 \n",
    "# 通常输入数据的格式为 [timesteps,batch_size,input_data],其中input_data 表示当前RNN的输入. \n",
    "input = tf.random_normal(shape=[3, batch_size, 6], dtype=tf.float32)\n",
    "# 这里使用最基本的 RNNCell ,10 表示一个RNNCell中有多少个隐藏单元\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(10)\n",
    "# 第一个RNNCell的输入为,输入数据和一个初始化的隐含状态,这个隐含状态通常初始化为0\n",
    "#　初始化状态的大小就是 batch_size, 表明有 batch_size 个独立的状态\n",
    "init_state = cell.zero_state(batch_size, dtype=tf.float32)\n",
    "# output 表示每一个RNNCell 的输出,输出的形状为(3,4,10)\n",
    "# final_state 表示最后一个 RNNCell 的输出状态,输出形状为(4,10)\n",
    "output, final_state = tf.nn.dynamic_rnn(cell, input, initial_state=init_state, time_major=True)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(output).shape)\n",
    "    print(sess.run(final_state))\n",
    "    print(sess.run(final_state).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 堆叠RNNCell: MultiRNNCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多时候，单层 RNN 的能力有限，我们需要多层的 RNN。将 $x$ 输入第一层 RNN 的后得到隐层状态 $h$,这个隐层状态就相当于第二层 RNN 的输入，第二层 RNN 的隐层状态又相当于第三层 RNN 的输入，以此类推。在TensorFlow 中，可以使用 $tf.nn.rnn\\_cell.MultiRNNCell$ 函数对 RNNCell 进行堆叠，相应的示例程序如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 128)\n",
      "(<tf.Tensor 'cell_0/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>, <tf.Tensor 'cell_1/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>, <tf.Tensor 'cell_2/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# 每调用一次这个函数就返回一个BasicRNNCell\n",
    "def get_a_cell():\n",
    "    return tf.nn.rnn_cell.BasicRNNCell(num_units=128)\n",
    "# 用tf.nn.rnn_cell MultiRNNCell创建3层RNN\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)]) # 3层RNN\n",
    "# 得到的cell实际也是RNNCell的子类\n",
    "# 它的state_size是(128, 128, 128)\n",
    "# (128, 128, 128)并不是128x128x128的意思\n",
    "# 而是表示共有3个隐层状态，每个隐层状态的大小为128\n",
    "print(cell.state_size) # (128, 128, 128)\n",
    "# 使用对应的call函数\n",
    "inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\n",
    "h0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态\n",
    "output, h1 = cell.call(inputs, h0)\n",
    "print(h1) # tuple中含有3个32x128的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Tensorflow 中的 cell 类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 中定义了 5 个常用的 cell 类,都可以在  $tf.contrib.rnn.XXXCell$  中找到, 具体定义如下:\n",
    "+ BasicRNNCell:\n",
    "```python\n",
    "BasicRNNCell(num_units)\n",
    "num_units: 每个 cell 包含的隐含单元个数\n",
    "```\n",
    "+ BasicLSTMCell:\n",
    "```python\n",
    "BasicLSTMCell(num_units,forget_bias=1.0,activation=tanh)\n",
    "num_units: 每个 cell 包含的隐含单元个数\n",
    "forget_bias: 添加到 forget 门的偏置\n",
    "activation: 默认激活函数\n",
    "```\n",
    "+ LSTMCell:\n",
    "```python\n",
    "LSTMCell(num_units,use_peepholes=False,cell_cip=None,initializer=None,forget_bias=1.0)\n",
    "num_units: 每个 cell 包含的隐含单元个数\n",
    "cell_clip: 在输出前对 cell 状态按照给定值进行截断处理\n",
    "initializer: 指定初始化函数\n",
    "forget_bias: 添加到 forget 门的偏置\n",
    "```\n",
    "+ GRUCell:\n",
    "```python\n",
    "GRUCell(num_units)\n",
    "num_units: 每个 cell 包含的隐含单元个数\n",
    "```\n",
    "+ MultiRNNCell:\n",
    "```python\n",
    "MultiRNNCell(cells,state_is_tuple=True)\n",
    "cells: 一个 cell 列表,将列表中的 cell 一个个堆叠起来,如果使用 cells=[cell1,cell2],就是一共两层,数据经过 cell1 后还要经过 cell2\n",
    "state_is_tuple: 如果是 True 则返回 n-tuple,将cell的输出值与状态组成一个 tuple .其中,输出值的结构为:c=[batch_size,num_units],输出状态的结构为 h=[batch_size,num_units]\n",
    "```\n",
    "<font color='red'>注意</font><br/>\n",
    "在使用 MultiRNNCell 时,有些习惯的写法是 cells 参数直接使用 [cell]$\\times$n 来代表创建 n 层的 cell, 这种写法如果不使用作用域隔离,则会报编译错误,或者使用一个外层循环将 cell 一个个 append 进去来解决命名冲突."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 通过 cell 类构建 RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义好 cell 类之后,还需要将他们链接起来构成 RNN 网络. Tensorflow 中主要有以下积累构建网络的模式,构建函数主要在两个包中,$tf.nn$ 和 $tf.contrib$ ,其中 $tf.contrib$ 表示由其他人提供的,而 $tf.nn$ 表示官方提供的稳定版:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 静态 RNN 构建\n",
    "Tensorflow 中提供了一个构建静态 RNN 的函数 $tf.nn.static\\_rnn$, 定义如下:\n",
    "```python\n",
    "tf.nn.static_rnn(cell,inputs,initial_state=None,dtype=None,sequence_length=None,scope=None)\n",
    "cell: 由 cell 类生成的对象\n",
    "inputs: 输入数据,一般数据输入格式为(timesteps,batch_size,cell_data)\n",
    "initial_state: 初始化 cell 状态\n",
    "dtype: 期望输出和初始化 state 的类型\n",
    "sequence_length: 每一个输入的序列长度,如果超过了 timesteps 就会截断,这也是静态 RNN 的一个缺点\n",
    "返回值: 一个是结果,一个是 cell 状态,结果是一个list,输入是多少个时序,list 里面就会有多少个元素.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 动态 RNN 构建\n",
    "Tensorflow 中提供了一个构建动态 RNN 的函数 $tf.nn.dynamic\\_rnn$, 定义如下:\n",
    "```python\n",
    "tf.nn.dynamic_rnn(cell,inputs,initial_state=None,dtype=None,sequence_length=None,scope=None)\n",
    "cell: 由 cell 类生成的对象\n",
    "inputs: 输入数据,一般数据输入格式为(batch_size,timesteps,cell_data)\n",
    "initial_state: 初始化 cell 状态\n",
    "dtype: 期望输出和初始化 state 的类型\n",
    "sequence_length: 每一个输入的序列长度,如果超过了 timesteps 就会截断,这也是静态 RNN 的一个缺点\n",
    "time_major: 默认值 False 时, input 的形状为 [batch_sise,timesteps,cell_data]. 如果是 True,形状为[timesteps,batch_size,cell_data]\n",
    "返回值: 一个是结果,一个是 cell 状态,结果是以 [batch_size,timesteps,...] 形式的张量.\n",
    "```\n",
    "<font color=\"red\">注意</font><br/>\n",
    "动态 RNN 的输出,它是以 batch_size 优先的矩阵,因为我们需要取最后一个时序的输出,所以需要处理成一个以时间优先的形式."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (3) 双向 RNN 构建\n",
    "双向 RNN 作为一个可以学习正,反向归来的循环神经网络,在 Tensorflow 中有3个函数可以使用:\n",
    "+ tf.nn.bidirectional_dynamic_rnn\n",
    "```python\n",
    "bidirectional_dynamic_rnn(cell_fw,cell_bw,inputs,initial_states_fw,initial_states_bw)\n",
    "cell_fw,cell_bw: 前向和反向的 Cell\n",
    "inputs: 输入序列,一个张量输入,形状为[batch_size,timesteps,cell_data]\n",
    "initial_states_fw,initial_states_bw: 前向和反向 cell 的初始状态\n",
    "#\n",
    "返回值: 是一个tuple(outputs,output_states),其中outputs也是一个\n",
    "tuple(output_fw,output_bw),每个值是一个张量 [batch_size,timesteps,layers_output],如果需要总的结果,可以将前后项的 layers_output 使用tf.concat 连接起来\n",
    "```\n",
    "\n",
    "+ tf.contrib.rnn.stack_bidirectional_rnn\n",
    "```python\n",
    "创建一个静态的多层双向网络.输出作为下一层的输入,前后向的输入大小必须一致,两层之间是独立的,不能共享信息.\n",
    "stack_bidirectional_rnn(cells_fw,cells_bw,inputs,initial_states_fw,initial_states_bw)\n",
    "cells_fw,cells_bw: 前向和后向实例化之后的 cell 列表,正反向的 list 长度必须相等,输入必须相同\n",
    "inputs: 输入为 [timesteps,batch_size,input_size] 形状的张量\n",
    "initial_states_fw,initial_states_bw: 前向和后向的 cell 初始化状态\n",
    "#\n",
    "返回值: 是一个tuple(outputs,output_states),其中outputs为 \n",
    "[timesteps,batch_size,layers_output] 形状的张量, layers_output 包含 tf.concat 之后的正反向的输出\n",
    "```\n",
    "\n",
    "+ tf.contrib.rnn.stack_bidirectional_dynamic_rnn\n",
    "```python\n",
    "创建一个动态的多层双向网络.输出作为下一层的输入,前后向的输入大小必须一致,两层之间是独立的,不能共享信息.\n",
    "stack_bidirectional_dynamic_rnn(cells_fw,cells_bw,inputs,initial_states_fw,initial_states_bw)\n",
    "cells_fw,cells_bw: 前向和后向实例化之后的 cell 列表,正反向的 list 长度必须相等,输入必须相同\n",
    "inputs: 输入形状为 [batch_size,timesteps,input_size] 形状的张量,与静态的关键不同\n",
    "initial_states_fw,initial_states_bw: 前向和后向的 cell 初始化状态\n",
    "#\n",
    "返回值: 是一个tuple(outputs,output_states),其中outputs为 \n",
    "[batch_size,timesteps,layers_output] 形状的张量, layers_output 包含 tf.concat 之后的正反向的输出\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>注意</font><br/>\n",
    "在单层,双层,双向 RNN 函数的介绍中,都有动态和静态之分.静态的意思就是按照样本的时间序列个数 $n$ 展开,在图中创建 $n$ 个序列的 cell. 动态的意思是只创建样本中一个序列的 RNN,其他的序列数据都会通过循环来进入该 RNN 来运算.\n",
    "\n",
    "通过静态生成的 RNN 网络,生成过程所需的时间比较长,网络所占有的内存会更多,导出的模型会更大.模型中会带有每个序列中间的状态信息,便于调试.并且所有样本的序列长度固定,过短需要填充,过长就会截断. 通过动态生成的 RNN 网络,所占的内存比较小,导出的模型较小.模型中只会有最后的状态,在使用过程中,支持不同的序列个数."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 使用动态 RNN 处理变长序列\n",
    "动态 RNN 还有个更高级的功能,就是可以处理变长序列,方法就是:<font color='red'>在准备样本的同时,将样本对应的长度也作为初始化参数,一起创建动态 RNN</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.2893718  -1.3948148   0.58520323  0.97595936  1.9312968 ]\n",
      "  [ 1.5639083   0.6806277  -0.39227822 -1.2727726  -0.6733243 ]\n",
      "  [-1.7387964  -2.4447942  -0.61489654 -0.0258193   0.04097625]\n",
      "  [ 0.2515934   1.9033813  -0.61183923  0.9252508  -0.76407474]]\n",
      "\n",
      " [[ 0.5319435   0.621407    0.14989032  0.45473146  1.1019354 ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = np.random.randn(2,4,5)\n",
    "x = x.astype(np.float32)\n",
    "x[1,1:]=0\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_seq:\n",
      "[[-0.08356715 -0.09119065  0.014137  ]\n",
      " [-0.0247704   0.18466851 -0.10629261]\n",
      " [-0.17865723 -0.18755475 -0.10416328]\n",
      " [ 0.15664953 -0.05130756  0.10724574]]\n",
      "short_seq:\n",
      "[[-0.02775398  0.13164161  0.0717001 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "lstm_cell_state:\n",
      "2\n",
      "[[ 0.15664953 -0.05130756  0.10724574]\n",
      " [-0.02775398  0.13164161  0.0717001 ]]\n",
      "gru_short_cell_state:\n",
      "[[-0.0616577   0.01478057 -0.07920398]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "gru_cell_state:\n",
      "2\n",
      "[-0.0616577   0.01478057 -0.07920398]\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [4,1]\n",
    "# 分别建立一个 lstm 和 gru 的 cell,比较两者的输出状态\n",
    "# staste_is_tuple 表示将输出状态和输出结果合并,变成(状态,输出值)\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=3,state_is_tuple=True)\n",
    "gru = tf.contrib.rnn.GRUCell(num_units=3)\n",
    "\n",
    "# 如果没有 initial_state,必须指定 dtype\n",
    "outputs,last_states = tf.nn.dynamic_rnn(cell,x,sequence_length=seq_lengths,dtype=tf.float32)\n",
    "gru_outputs,gru_last_states = tf.nn.dynamic_rnn(gru,x,sequence_length=seq_lengths,dtype=tf.float32)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "output,cell_state,gru_output,gru_state = sess.run([outputs,last_states,gru_outputs,gru_last_states])\n",
    "print(\"full_seq:\")\n",
    "print(output[0])\n",
    "print(\"short_seq:\")\n",
    "print(output[1])\n",
    "print(\"lstm_cell_state:\")\n",
    "print(len(cell_state))\n",
    "print(cell_state[1])\n",
    "print(\"gru_short_cell_state:\")\n",
    "print(gru_output[1])\n",
    "print(\"gru_cell_state:\")\n",
    "print(len(gru_state))\n",
    "print(gru_state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 构建单层静态/动态 LSTM 网络对 MNIST 数据集分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "('epochs:', 1, 'loss:', 0.32108588239008684, 'acc:', 0.9754)\n",
      "('epochs:', 2, 'loss:', 0.08950058505616408, 'acc:', 0.9813)\n",
      "('epochs:', 3, 'loss:', 0.06395129573235121, 'acc:', 0.98)\n",
      "('epochs:', 4, 'loss:', 0.05754251313522797, 'acc:', 0.9816)\n",
      "('epochs:', 5, 'loss:', 0.04936414777872744, 'acc:', 0.9797)\n",
      "('epochs:', 6, 'loss:', 0.04605702987967314, 'acc:', 0.9829)\n",
      "('epochs:', 7, 'loss:', 0.04210281130963601, 'acc:', 0.978)\n",
      "('epochs:', 8, 'loss:', 0.041488120896022086, 'acc:', 0.9858)\n",
      "('epochs:', 9, 'loss:', 0.03870155098096634, 'acc:', 0.9847)\n",
      "('epochs:', 10, 'loss:', 0.04109361199640925, 'acc:', 0.9789)\n",
      "('epochs:', 11, 'loss:', 0.0406065006763674, 'acc:', 0.9834)\n",
      "('epochs:', 12, 'loss:', 0.03408122316129846, 'acc:', 0.9843)\n",
      "('epochs:', 13, 'loss:', 0.038966907959812426, 'acc:', 0.9854)\n",
      "('epochs:', 14, 'loss:', 0.03396560745012142, 'acc:', 0.9871)\n",
      "('epochs:', 15, 'loss:', 0.028564826215892538, 'acc:', 0.9862)\n",
      "('epochs:', 16, 'loss:', 0.032227421292971625, 'acc:', 0.9838)\n",
      "('epochs:', 17, 'loss:', 0.038961409904507235, 'acc:', 0.9822)\n",
      "('epochs:', 18, 'loss:', 0.03564198308962991, 'acc:', 0.9852)\n",
      "('epochs:', 19, 'loss:', 0.039350004824703855, 'acc:', 0.9804)\n",
      "('epochs:', 20, 'loss:', 0.039767288955309524, 'acc:', 0.9841)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist/\",one_hot = True)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_input = 28\n",
    "n_steps = 28\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,n_steps*n_input])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,n_classes])\n",
    "\n",
    "# 如果使用 static_rnn 就需要将输入转换成 [timesteps,batch_size,cell_data]\n",
    "# 如果使用 dynamic_rnn 就不需要转换\n",
    "# tf.unstack 表示将输入x 按照 axis=1 拆分成 n_steps 个元素\n",
    "x_1 = tf.reshape(x,[-1,n_steps,n_input])\n",
    "x_2 = tf.unstack(x_1,n_steps,axis=1)\n",
    "# 由于 static_rnn 的输入数据必须是一个 list 所以不能直接使用 tf.transpose 转换\n",
    "# x_2 = tf.transpose(x_1,[1,0,2])\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_hidden)\n",
    "# gru_cell = tf.contrib.rnn.GRUCell(num_units=n_hidden)\n",
    "outputs,states = tf.nn.static_rnn(cell=lstm_cell,inputs=x_2,dtype=tf.float32)\n",
    "# 静态 RNN 的输出,形状为[timesteps,batch_size,...],所以直接取最后一步的结果就可以了\n",
    "# 注意动态的输入数据格式为[batch_size,timesteps,cell_data]\n",
    "# outputs,states = tf.nn.dynamic_rnn(cell=lstm_cell,inputs=x_1,dtype=tf.float32)\n",
    "# 动态 RNN 的输出,形状为[batch_size,timesteps,...],需要转换结果,变成[timesteps,batch_size,...]\n",
    "# 一般可以使用 tf.transpose 转换,可以直接换位置,或者也可以使用 tf.unstack\n",
    "# outputs = tf.transpose(outputs,[1,0,2])\n",
    "pred = tf.contrib.layers.fully_connected(outputs[-1],n_classes,activation_fn=None)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "train_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(train_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for j in range(total_batch):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "            _,loss = sess.run([optimizer,cost],feed_dict={x:batch_x,y:batch_y})\n",
    "            avg_cost = avg_cost+loss/total_batch\n",
    "        if (i+1)%display_step==0:\n",
    "            # 如果两者最大值索引相等,那么就会该位置就会变成True,否则变成False\n",
    "            correct_prediction = tf.equal(tf.argmax(pred,axis=1),tf.argmax(y,axis=1))\n",
    "            acc = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "            accuracy = sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "            print('epochs:',i+1,'loss:',avg_cost,'acc:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 构建多层静态/动态 LSTM 网络对 MNIST 数据集分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层 RNN 在创建过程中,需要使用到前面介绍的 MultiRNNCell ,这个类实例化需要通过单层的 cell 对象输入.\n",
    "与前面的例子类似,先创建单层的 cell,然后再创建 MultiRNNCell 对象,在创建好 MultiRNNCell 后,可以通过静态或动态的 RNN 网络组合."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "('epochs:', 1, 'loss:', 0.382035752863369, 'acc:', 0.96)\n",
      "('epochs:', 2, 'loss:', 0.12375072764910074, 'acc:', 0.9723)\n",
      "('epochs:', 3, 'loss:', 0.10376593740741642, 'acc:', 0.969)\n",
      "('epochs:', 4, 'loss:', 0.0962252793715082, 'acc:', 0.9777)\n",
      "('epochs:', 5, 'loss:', 0.09086501511745156, 'acc:', 0.9783)\n",
      "('epochs:', 6, 'loss:', 0.09430076590569858, 'acc:', 0.9725)\n",
      "('epochs:', 7, 'loss:', 0.09274781296961002, 'acc:', 0.9713)\n",
      "('epochs:', 8, 'loss:', 0.13535508378578184, 'acc:', 0.9561)\n",
      "('epochs:', 9, 'loss:', 0.13893080826002097, 'acc:', 0.9292)\n",
      "('epochs:', 10, 'loss:', 0.9730930625579577, 'acc:', 0.7425)\n",
      "('epochs:', 11, 'loss:', 0.4223040672865779, 'acc:', 0.9202)\n",
      "('epochs:', 12, 'loss:', 0.23397439456121472, 'acc:', 0.9319)\n",
      "('epochs:', 13, 'loss:', 0.1646895441683856, 'acc:', 0.9512)\n",
      "('epochs:', 14, 'loss:', 0.13739926032383326, 'acc:', 0.9526)\n",
      "('epochs:', 15, 'loss:', 0.12152116633443662, 'acc:', 0.9701)\n",
      "('epochs:', 16, 'loss:', 0.11234938312491237, 'acc:', 0.9712)\n",
      "('epochs:', 17, 'loss:', 0.10657794650813389, 'acc:', 0.9743)\n",
      "('epochs:', 18, 'loss:', 0.09913375679742208, 'acc:', 0.971)\n",
      "('epochs:', 19, 'loss:', 0.10247761863029817, 'acc:', 0.9736)\n",
      "('epochs:', 20, 'loss:', 0.09015606756999407, 'acc:', 0.9777)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist/\",one_hot = True)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_input = 28\n",
    "n_steps = 28\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,n_steps*n_input])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,n_classes])\n",
    "\n",
    "# 如果使用 static_rnn 就需要将输入转换成 [timesteps,batch_size,cell_data]\n",
    "# 如果使用 dynamic_rnn 就不需要转换\n",
    "# tf.unstack 表示将输入x 按照 axis=1 拆分成 n_steps 个元素\n",
    "x_1 = tf.reshape(x,[-1,n_steps,n_input])\n",
    "x_2 = tf.unstack(x_1,n_steps,axis=1)\n",
    "# 由于 static_rnn 的输入数据必须是一个 list 所以不能直接使用 tf.transpose 转换\n",
    "# x_2 = tf.transpose(x_1,[1,0,2])\n",
    "\n",
    "# 不能直接使用 [cell]*3 会导致作用域冲突\n",
    "stack_rnn = []\n",
    "for i in range(3):\n",
    "    stack_rnn.append(tf.contrib.rnn.LSTMCell(num_units=n_hidden))\n",
    "# 使用 MultiRNNCell 构建多层\n",
    "mcell = tf.contrib.rnn.MultiRNNCell(stack_rnn)\n",
    "outputs,states = tf.nn.static_rnn(cell=mcell,inputs=x_2,dtype=tf.float32)\n",
    "# 静态 RNN 的输出,形状为[timesteps,batch_size,...],所以直接取最后一步的结果就可以了\n",
    "# 注意动态的输入数据格式为[batch_size,timesteps,cell_data]\n",
    "# outputs,states = tf.nn.dynamic_rnn(cell=lstm_cell,inputs=x_1,dtype=tf.float32)\n",
    "# 动态 RNN 的输出,形状为[batch_size,timesteps,...],需要转换结果,变成[timesteps,batch_size,...]\n",
    "# 一般可以使用 tf.transpose 转换,可以直接换位置,或者也可以使用 tf.unstack\n",
    "# outputs = tf.transpose(outputs,[1,0,2])\n",
    "pred = tf.contrib.layers.fully_connected(outputs[-1],n_classes,activation_fn=None)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "train_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(train_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for j in range(total_batch):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "            _,loss = sess.run([optimizer,cost],feed_dict={x:batch_x,y:batch_y})\n",
    "            avg_cost = avg_cost+loss/total_batch\n",
    "        if (i+1)%display_step==0:\n",
    "            # 如果两者最大值索引相等,那么就会该位置就会变成True,否则变成False\n",
    "            correct_prediction = tf.equal(tf.argmax(pred,axis=1),tf.argmax(y,axis=1))\n",
    "            acc = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "            accuracy = sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "            print('epochs:',i+1,'loss:',avg_cost,'acc:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 构建单层静态/动态双向 RNN 网络对 MNIST 数据集分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先建立两个 cell ,每个 cell 包含 128 个隐含单元,然后使用 $tf.nn.bidirection\\_dynamic\\_rnn$ 函数将 $x$ 放进去生成节点 outputs, 由于 bidirectional_dynamic_rnn 的输出结果与状态是分离的, 所以需要手动将结果合并起来并进行转置, 然后通过全连接层生成 pred,再使用 softmax 进行分类."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "('epochs:', 1, 'loss:', 0.32167011274363516, 'acc:', 0.9692)\n",
      "('epochs:', 2, 'loss:', 0.09290599693036218, 'acc:', 0.9767)\n",
      "('epochs:', 3, 'loss:', 0.07222473755648195, 'acc:', 0.9804)\n",
      "('epochs:', 4, 'loss:', 0.06027137368028478, 'acc:', 0.9783)\n",
      "('epochs:', 5, 'loss:', 0.04935369869757609, 'acc:', 0.9819)\n",
      "('epochs:', 6, 'loss:', 0.046610671895429105, 'acc:', 0.9838)\n",
      "('epochs:', 7, 'loss:', 0.0426436970689842, 'acc:', 0.9825)\n",
      "('epochs:', 8, 'loss:', 0.03908720361746166, 'acc:', 0.9853)\n",
      "('epochs:', 9, 'loss:', 0.04210371764768338, 'acc:', 0.9829)\n",
      "('epochs:', 10, 'loss:', 0.041152892129347536, 'acc:', 0.9815)\n",
      "('epochs:', 11, 'loss:', 0.0383973893146454, 'acc:', 0.9847)\n",
      "('epochs:', 12, 'loss:', 0.04132545971269298, 'acc:', 0.9843)\n",
      "('epochs:', 13, 'loss:', 0.03511075249407445, 'acc:', 0.9833)\n",
      "('epochs:', 14, 'loss:', 0.03226190004106188, 'acc:', 0.9854)\n",
      "('epochs:', 15, 'loss:', 0.03639015693566763, 'acc:', 0.9836)\n",
      "('epochs:', 16, 'loss:', 0.03774884424955503, 'acc:', 0.9831)\n",
      "('epochs:', 17, 'loss:', 0.027137027995936088, 'acc:', 0.984)\n",
      "('epochs:', 18, 'loss:', 0.030827764496471812, 'acc:', 0.9799)\n",
      "('epochs:', 19, 'loss:', 0.039836990014247765, 'acc:', 0.9832)\n",
      "('epochs:', 20, 'loss:', 0.035181173463034544, 'acc:', 0.9787)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist/\",one_hot = True)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_input = 28\n",
    "n_steps = 28\n",
    "n_hidden = 128\n",
    "n_classes = 10\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,n_steps*n_input])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,n_classes])\n",
    "x_1 = tf.reshape(x,[-1,n_steps,n_input])\n",
    "# 前向cell\n",
    "lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden)\n",
    "# 反向cell\n",
    "lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_hidden)\n",
    "# 构建双向 RNN 的函数\n",
    "outputs,states = tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,cell_bw=lstm_bw_cell,\n",
    "                                                 inputs=x_1,dtype=tf.float32)\n",
    "# 将输出结果的那一个维度合并起来\n",
    "outputs = tf.concat(outputs,axis=2)\n",
    "outputs = tf.transpose(outputs,[1,0,2])\n",
    "pred = tf.contrib.layers.fully_connected(outputs[-1],n_classes,activation_fn=None)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "train_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(train_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for j in range(total_batch):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "            _,loss = sess.run([optimizer,cost],feed_dict={x:batch_x,y:batch_y})\n",
    "            avg_cost = avg_cost+loss/total_batch\n",
    "        if (i+1)%display_step==0:\n",
    "            # 如果两者最大值索引相等,那么就会该位置就会变成True,否则变成False\n",
    "            correct_prediction = tf.equal(tf.argmax(pred,axis=1),tf.argmax(y,axis=1))\n",
    "            acc = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "            accuracy = sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "            print('epochs:',i+1,'loss:',avg_cost,'acc:',accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>循环神经网络</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 循环神经网络介绍\n",
    "举个例子,很多课文我们可以顺序背诵,而不能倒序背诵.这种现象可以理解为我们大脑并不是简单的存储,而是链式的,有序的存储.这种存储方式很节省存储空间,对于中间状态的序列,没有直接记住,而是记住了计算方法.\n",
    "<center><img src='./img/7/1.png' width='600'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 RNN实现退位减法器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 建立基本函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义 sigmoid 函数\n",
    "def sigmoid(x):\n",
    "    output = 1./(1+np.exp(-x))\n",
    "    return output\n",
    "# 定义 sigmoid 函数的导数\n",
    "def sigmoid_derivative(output):\n",
    "    return output*(1.-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)建立二进制映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个这样的字典:{0:00000000,1:00000001,2:00000010,...,255:11111111}\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "largest_number = np.power(2,binary_dim)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = bin(i)[2:].zfill(binary_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)定义参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义学习参数:隐含层的权重为 synapse_0,循环节点的权重为 synapse_h(输入节点16,输出节点16),输出层的权重为synapse_1(输入16节点,输出1节点)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.9 # 学习率\n",
    "input_dim = 2       # 输入的维度是2,减数和被减数\n",
    "hidden_dim = 16 \n",
    "output_dim =1       # 输出维度\n",
    "\n",
    "# 初始化网络\n",
    "# 输入维度是2,隐藏维度是16,为了保证初始化参数的范围 [-0.05,0.05]\n",
    "synapse_0 = (2*np.random.random((input_dim,hidden_dim))-1)*0.05\n",
    "synapse_1 = (2*np.random.random((hidden_dim,output_dim))-1)*0.05\n",
    "synapse_h = (2*np.random.random((hidden_dim,hidden_dim))-1)*0.05\n",
    "\n",
    "# 用于存放反向传播的权重更新值\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 准备样本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大致是这样的过程:\n",
    "+ 建立循环生成样本数据,先生成两个数 a 和 b .如果 a<b 就交换位置.\n",
    "+ 计算出相减的结果 c\n",
    "+ 将三个数转换成二进制."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10000):\n",
    "    a_int = np.random.randint(largest_number)\n",
    "    b_int = np.random.randint(largest_number)\n",
    "    if a_int < b_int:\n",
    "        t = b_int\n",
    "        b_int = a_int\n",
    "        a_int = t\n",
    "    # 将三个数都转换为对应的二进制\n",
    "    a = int2binary[a_int]\n",
    "    b = int2binary[b_int]\n",
    "    c = int2binary[a_int-b_int]\n",
    "    # 模型初始化输出值为0,初始化总误差为0,定义 layer_deltas 存储反向传播过程中的循环层的误差,\n",
    "    # layer_values 为隐含层的输出值,由于第一个数据传入是传入时,没有前面的隐含层输出值作为本次\n",
    "    # 样本的输入,所以需要定义一个初始值,这里定义为0.1\n",
    "    d = np.zeros_like(np.arange(8))\n",
    "    overallError = 0\n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    # 一开始没有隐含层,所以初始化一下原始值为0.1\n",
    "    layer_1_values.append(np.ones(hidden_dim)*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) 正向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环遍历每个二进制位,从个位开始依次相减,并将中间隐藏层的输出传入下一个时间步,把每一个时间步的误差导数记录下来,同时统计总误差."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for position in range(binary_dim):\n",
    "        # 生成输入,从右到左,每次取两个输入数字的一个bit位\n",
    "        x = np.array([[int(a[binary_dim-position-1]),int(b[binary_dim-position-1])]])\n",
    "        y = np.array([[int(c[binary_dim-position-1])]])\n",
    "        # 隐含层的输入,包含上一步的隐藏层和当层的输入,输入层需要经过sigmoid只有才能相加\n",
    "        # (输入层+之前的隐藏层)->新的隐藏层\n",
    "        layer_1 = sigmoid(np.dot(x,synapse_0))+np.dot(layer_1_values[-1],synapse_h)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append(layer_2_error*sigmoid_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0]) #总误差\n",
    "        # 记录每次预测的bit位\n",
    "        d[binary_dim-position-1] = np.round(layer_2[0][0])\n",
    "        # 将当前的隐藏层保存下来,下个时间序列用\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (6) 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播是从最后一次往前反向计算误差,对于每一个当前的计算都需要下一次结果参与,反向计算从最后一次开始,它没有后一次的输入,所以需要初始化一个值作为其后一次的输入,这里初始化为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    # position=[0,1,2,3,4,5,6,7]\n",
    "    for position in range(binary_dim):\n",
    "        # 最后一次的两个输入\n",
    "        x = np.array([[int(a[position]),int(b[position])]])\n",
    "        # 当前时间点的隐藏层\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        # 上一个时间点的隐藏层\n",
    "        pre_layer_1 = layer_1_values[-position-2]\n",
    "        # 当前时间点的输出层误差\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # 通过后一个时间点的隐藏层误差和当前时间点的输出层误差,计算当前时间点的隐藏层误差\n",
    "        layer_1_delta = (np.dot(future_layer_1_delta,synapse_h.T)+\n",
    "                         np.dot(layer_2_delta,synapse_1.T))*sigmoid_derivative(layer_1)\n",
    "        # 等完成所有反向传播误差计算,才会更新权重矩阵,先暂时把更新矩阵存起来\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(pre_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += x.T.dot(layer_1_delta)\n",
    "        future_layer_1_delta = layer_1_delta\n",
    "\n",
    "    # 完成所有反向传播之后,更新权重矩阵,并把矩阵变量清零\n",
    "    synapse_0 += synapse_0_update*learning_rate\n",
    "    synapse_1 += synapse_1_update*learning_rate\n",
    "    synapse_h += synapse_h_update*learning_rate\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (7) 代码汇总"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>多层神经网络</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "gpu_no = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_no\n",
    "# 定义TensorFlow配置\n",
    "config = tf.ConfigProto()\n",
    "# 配置GPU内存分配方式，按需增长，很关键\n",
    "config.gpu_options.allow_growth = True\n",
    "# 配置可使用的显存比例\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
    "# 在创建session的时候把config作为参数传进去\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 使用隐藏层解决非线性问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32,shape=[None,2])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,1])\n",
    "\n",
    "# 定义隐含层1\n",
    "h1 = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[2,3]))\n",
    "h2 = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[3,1]))\n",
    "# 定义偏置\n",
    "b1 = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[3]))\n",
    "b2 = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[1]))\n",
    "\n",
    "# 定义网络模型\n",
    "layer_1 = tf.nn.relu(tf.matmul(x,h1)+b1)\n",
    "pred = tf.nn.sigmoid(tf.matmul(layer_1,h2)+b2)\n",
    "cost = tf.reduce_mean(tf.square(y-pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [[0],[1],[1],[0]]\n",
    "X = np.array(X).astype(np.float32)\n",
    "Y = np.array(Y).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00143438]\n",
      " [0.99931204]\n",
      " [0.9995128 ]\n",
      " [0.00143438]]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(10000):\n",
    "    sess.run(optimizer,feed_dict={x:X,y:Y})\n",
    "print(sess.run(pred,feed_dict={x:X}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 使用全连接网络对图片分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-9ffb167caaf3>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/xujun/anaconda2/envs/python35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "epochs: 1 loss: 28.88468648585407 acc: 0.9295\n",
      "epochs: 2 loss: 5.447905037635568 acc: 0.9423\n",
      "epochs: 3 loss: 2.8284919566736506 acc: 0.9476\n",
      "epochs: 4 loss: 1.8984966477905423 acc: 0.9447\n",
      "epochs: 5 loss: 1.362716485215845 acc: 0.9533\n",
      "epochs: 6 loss: 1.2237135898893499 acc: 0.9588\n",
      "epochs: 7 loss: 1.0794722994841945 acc: 0.9548\n",
      "epochs: 8 loss: 0.9170498244012731 acc: 0.957\n",
      "epochs: 9 loss: 0.9084995211146217 acc: 0.9584\n",
      "epochs: 10 loss: 0.7410662257733639 acc: 0.9531\n",
      "epochs: 11 loss: 0.5989777556077995 acc: 0.9601\n",
      "epochs: 12 loss: 0.6129861723021847 acc: 0.9596\n",
      "epochs: 13 loss: 0.4263177204875773 acc: 0.9544\n",
      "epochs: 14 loss: 0.3498359471277133 acc: 0.9598\n",
      "epochs: 15 loss: 0.35863572429415125 acc: 0.9571\n",
      "epochs: 16 loss: 0.3082335597528941 acc: 0.9537\n",
      "epochs: 17 loss: 0.26772726118414536 acc: 0.9618\n",
      "epochs: 18 loss: 0.23572992223550876 acc: 0.9603\n",
      "epochs: 19 loss: 0.1833578888584717 acc: 0.9583\n",
      "epochs: 20 loss: 0.1797526859569592 acc: 0.9552\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist/\",one_hot = True)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,784])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,10])\n",
    "\n",
    "h1 = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[784,256]))\n",
    "b1 = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[256]))\n",
    "\n",
    "h2 = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[256,256]))\n",
    "b2 = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[256]))\n",
    "\n",
    "h_out = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[256,10]))\n",
    "b_out = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[10]))\n",
    "\n",
    "layer_1 = tf.nn.relu(tf.matmul(x,h1)+b1)\n",
    "layer_2 = tf.nn.relu(tf.matmul(layer_1,h2)+b2)\n",
    "layer_out = tf.matmul(layer_2,h_out)+b_out\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_out,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "train_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(train_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for j in range(total_batch):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "            _,loss = sess.run([optimizer,cost],feed_dict={x:batch_x,y:batch_y})\n",
    "            avg_cost = avg_cost+loss/total_batch\n",
    "        if (i+1)%display_step==0:\n",
    "            # 如果两者最大值索引相等,那么就会该位置就会变成True,否则变成False\n",
    "            correct_prediction = tf.equal(tf.argmax(layer_out,axis=1),tf.argmax(y,axis=1))\n",
    "            acc = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "            accuracy = sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "            print('epochs:',i+1,'loss:',avg_cost,'acc:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所谓的正则化,其实就是在神经网络计算损失值的过程中,在损失后面再加一项.随着模型复杂度增加,那么正则化损失也会增加,这样就能防止过拟合.\n",
    "+ $L1$损失:所有学习参数 $w$ 的绝对值的和.\n",
    "+ $L2$损失:所欲学习参数 $w$ 的平方和然后就平方根\n",
    "\n",
    "```python\n",
    "Tensorflow 中 L2 正则化函数为:\n",
    "tf.nn.l2_loss(w,name=None)\n",
    "\n",
    "Tensorflow 中没有现成的 L1 正则化函数,需要自己组合\n",
    "tf.reduce_sum(tf.abs(w))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "epochs: 1 loss: 103.19655143737795 acc: 0.9278\n",
      "epochs: 2 loss: 58.23222935763277 acc: 0.9389\n",
      "epochs: 3 loss: 41.8394661088423 acc: 0.9445\n",
      "epochs: 4 loss: 30.256434114629588 acc: 0.9408\n",
      "epochs: 5 loss: 21.46111306277189 acc: 0.9509\n",
      "epochs: 6 loss: 14.805812995217059 acc: 0.95\n",
      "epochs: 7 loss: 9.72043402411721 acc: 0.9527\n",
      "epochs: 8 loss: 6.158308019638061 acc: 0.952\n",
      "epochs: 9 loss: 3.612032660137522 acc: 0.9595\n",
      "epochs: 10 loss: 1.9632780209454623 acc: 0.9615\n",
      "epochs: 11 loss: 1.0657167414101694 acc: 0.9593\n",
      "epochs: 12 loss: 0.6343103901364584 acc: 0.962\n",
      "epochs: 13 loss: 0.4481143177639356 acc: 0.9525\n",
      "epochs: 14 loss: 0.36888129472732567 acc: 0.9554\n",
      "epochs: 15 loss: 0.34787350337613737 acc: 0.9539\n",
      "epochs: 16 loss: 0.33314223213629285 acc: 0.9531\n",
      "epochs: 17 loss: 0.3303274585171174 acc: 0.9475\n",
      "epochs: 18 loss: 0.33278046843680464 acc: 0.957\n",
      "epochs: 19 loss: 0.33091571149500965 acc: 0.9425\n",
      "epochs: 20 loss: 0.32800861429084455 acc: 0.9344\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist/\",one_hot = True)\n",
    "tf.reset_default_graph()shuchu\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,784])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,10])\n",
    "\n",
    "h1 = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[784,256]))\n",
    "b1 = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[256]))\n",
    "\n",
    "h2 = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[256,256]))\n",
    "b2 = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[256]))\n",
    "\n",
    "h_out = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[256,10]))\n",
    "b_out = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[10]))\n",
    "\n",
    "layer_1 = tf.nn.relu(tf.matmul(x,h1)+b1)\n",
    "layer_2 = tf.nn.relu(tf.matmul(layer_1,h2)+b2)\n",
    "layer_out = tf.matmul(layer_2,h_out)+b_out\n",
    "\n",
    "# 在这个数据集中,过拟合现象不严重,所以加入之后对效果影响不大\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_out,labels=y))\n",
    "cost = cost + tf.nn.l2_loss(h1)*0.001+tf.nn.l2_loss(h2)*0.001+tf.nn.l2_loss(h_out)*0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "train_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(train_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for j in range(total_batch):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "            _,loss = sess.run([optimizer,cost],feed_dict={x:batch_x,y:batch_y})\n",
    "            avg_cost = avg_cost+loss/total_batch\n",
    "        if (i+1)%display_step==0:\n",
    "            # 如果两者最大值索引相等,那么就会该位置就会变成True,否则变成False\n",
    "            correct_prediction = tf.equal(tf.argmax(layer_out,axis=1),tf.argmax(y,axis=1))\n",
    "            acc = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "            accuracy = sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels})\n",
    "            print('epochs:',i+1,'loss:',avg_cost,'acc:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "dropout在每次训练过程中,随机选择一部分节点不去学习.过拟合的问题恰恰是把异常数据当成归来来学习了,所以dropout会忽略一些节点,对于量比较大的正样本忽略了,不碍事,还有很多.对于异常样本,忽略了就有利于模型的效果.\n",
    "\n",
    "dropout 改变了神经网络的结构,它仅仅是属于训练时的方法,所以一般在进行测试时要将 dropout 的 keep_prob 变为 1 ,代表不需要丢弃,否则影响模型的正常输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "epochs: 1 loss: 126.09144298206687 acc: 0.9237\n",
      "epochs: 2 loss: 63.4208103942871 acc: 0.926\n",
      "epochs: 3 loss: 45.32155839399856 acc: 0.9109\n",
      "epochs: 4 loss: 31.962404528531174 acc: 0.9107\n",
      "epochs: 5 loss: 22.071054656288837 acc: 0.9328\n",
      "epochs: 6 loss: 14.551033335599032 acc: 0.9456\n",
      "epochs: 7 loss: 9.062510582317014 acc: 0.9534\n",
      "epochs: 8 loss: 5.297731639255179 acc: 0.9562\n",
      "epochs: 9 loss: 2.9327388191223145 acc: 0.9599\n",
      "epochs: 10 loss: 1.5967770346728243 acc: 0.9606\n",
      "epochs: 11 loss: 0.9326776024428278 acc: 0.952\n",
      "epochs: 12 loss: 0.6448689064654438 acc: 0.9553\n",
      "epochs: 13 loss: 0.5292343805053019 acc: 0.956\n",
      "epochs: 14 loss: 0.507958949804306 acc: 0.9455\n",
      "epochs: 15 loss: 0.5028879054026167 acc: 0.9542\n",
      "epochs: 16 loss: 0.5305594129995872 acc: 0.9501\n",
      "epochs: 17 loss: 0.533424191637473 acc: 0.9452\n",
      "epochs: 18 loss: 0.5393758358738645 acc: 0.9408\n",
      "epochs: 19 loss: 0.5328976633873853 acc: 0.9391\n",
      "epochs: 20 loss: 0.528039393370802 acc: 0.936\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist/\",one_hot = True)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32,shape=[None,784])\n",
    "y = tf.placeholder(dtype=tf.float32,shape=[None,10])\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "h1 = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[784,256]))\n",
    "b1 = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[256]))\n",
    "\n",
    "h2 = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[256,256]))\n",
    "b2 = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[256]))\n",
    "\n",
    "h_out = tf.Variable(dtype=tf.float32,initial_value=tf.truncated_normal(shape=[256,10]))\n",
    "b_out = tf.Variable(dtype=tf.float32,initial_value=tf.zeros(shape=[10]))\n",
    "\n",
    "layer_1 = tf.nn.relu(tf.matmul(x,h1)+b1)\n",
    "layer_1_dropout = tf.nn.dropout(x=layer_1,keep_prob=keep_prob)\n",
    "\n",
    "layer_2 = tf.nn.relu(tf.matmul(layer_1_dropout,h2)+b2)\n",
    "layer_2_dropout = tf.nn.dropout(x=layer_2,keep_prob=keep_prob)\n",
    "\n",
    "layer_out = tf.matmul(layer_2_dropout,h_out)+b_out\n",
    "\n",
    "# 在这个数据集中,过拟合现象不严重,所以加入之后对效果影响不大\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_out,labels=y))\n",
    "cost = cost + tf.nn.l2_loss(h1)*0.001+tf.nn.l2_loss(h2)*0.001+tf.nn.l2_loss(h_out)*0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "train_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(train_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        for j in range(total_batch):\n",
    "            batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "            # 训练时可以使用 dropout\n",
    "            _,loss = sess.run([optimizer,cost],feed_dict={x:batch_x,y:batch_y,keep_prob:0.8})\n",
    "            avg_cost = avg_cost+loss/total_batch\n",
    "        if (i+1)%display_step==0:\n",
    "            # 如果两者最大值索引相等,那么就会该位置就会变成True,否则变成False\n",
    "            correct_prediction = tf.equal(tf.argmax(layer_out,axis=1),tf.argmax(y,axis=1))\n",
    "            acc = tf.reduce_mean(tf.cast(correct_prediction,dtype=tf.float32))\n",
    "            # 测试时不能使用dropout\n",
    "            accuracy = sess.run(acc,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.})\n",
    "            print('epochs:',i+1,'loss:',avg_cost,'acc:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 全连接网络的深浅关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接网络,只要有足够多的神经元,即使只有一层隐含层的神经网络.利用常用的 sigmoid 和 relu 激活函数,就可以无限逼近任何连续函数.\n",
    "\n",
    "在实际中,如果需要使用浅层神经网络来拟合复杂的非线性函数,就需要依靠增加神经元的个数来实现. 神经元过多意味着需要训练的参数越多,训练速度比较慢,泛化能力差. <font color='red'> 一般倾向于使用更深的模型,来减少网络中所需神经元的数量,提升网络的泛化性能</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
